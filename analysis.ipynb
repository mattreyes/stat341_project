{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many statistical populations contain distinct subpopulations, where each one may posses its own density. This can result in a multimodal distribution which can not be modelled by a single density. Modelling mixtures of distributions has key applications in document topic analysis and financial crisis modelling.\n",
    "\n",
    "Gaussian Mixture Models, used to model subpopulations, are convex combinations of Gaussian densities. Parameters for this model can be estimated using the Expectation Maximization (EM) algorithm - a probablistic approach.\n",
    "\n",
    "Recent discoveries in optimization research have yielded genetic algorithms inspired by processes found in nature. One particular example is Particle Swarm Optimization, which we implement to perform maximum likelihood estimation. We compare this algorithm to the classical EM by applying them to the Old Faithful dataset and to simulated mixture densities.\n",
    "\n",
    "Particularly, we aim to compare this non-statistical approach (PSO) to the probablisitic approach (EM) to estimate densities for the univariate case where there are two distinct groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, datasets are generated from process which can be represented as a combination of Gaussian distributions. The parameters of the distribution that generates these samples can offer insights into properties of the population, such as the subpopulation means and the weights of each mixture component. Concretely, this mixture distribution represents a decomposition of a single density into a convex combination of many. \n",
    "\n",
    "We formally define a Gaussian Mixture Model pdf $f$ with two components as\n",
    "\n",
    "$$\n",
    "f(x\\space|\\space\\mu_{1},\\sigma_{1}^{2},\\mu_{2},\\sigma_{2}^{2}) = \\pi\\phi(x\\space|\\space\\mu_{1},\\sigma_{1}^{2}) + (1-\\pi)\\phi(x\\space|\\space\\mu_{2},\\sigma_{2}^{2})\n",
    "$$\n",
    "\n",
    "where $\\phi_{1},\\phi_{2}$ are the Gaussian pdf and the weight $\\pi \\in [0,1]$.\n",
    "\n",
    "This formulation describes two Gaussian components, each with a weight and their respective parameters $\\mu$ and $\\sigma_{2}$ being their mean and variance respectively.\n",
    "\n",
    "We often do not know these parameters of the population given a sample dataset. At first glance, an approach would be to determine the log-likelihood function, yielding\n",
    "\n",
    "\n",
    "$$\n",
    "l(\\mu_{1},\\sigma_{1}^{2},\\mu_{2},\\sigma_{2}^{2}|x) = \n",
    "\\sum_{i=1}^{n}[\\ln(\\pi\\phi(x_{i}|\\mu_{1},\\sigma_{1}^{2}) + (1-\\pi)\\phi(x_{i}|\\mu_{2},\\sigma_{2}^{2}))]\n",
    "$$\n",
    "\n",
    "Maximum likelihood estimation by setting the score function to zero would require solving a multivariate optimization problem. This problem is challenging due to multiple variables, the complexity of the density $\\phi$, and the summation of these. Thus numerical algorithms are required to find an approximate solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most widely used approach is the Expectation Maximization (EM) algorithm [3]. It is a deterministic optimization technique derived from minorize-maximization (MM) methods. We say the log-likelihood function is maximized by by iteratively maximizing simpler functions. \n",
    "\n",
    "We propose to compare this to the Particle Swarm Optimization - a stochastic optimization algorithm first described by Kennedy [1] in 1995. It draws inspirations from behaviour seen in crowds of lifeforms such as school of fish or flocks of birds. The main idea is to begin with an initial 'swarm' of particles with values randomly chosen from the search space. Each particle represents a candidate solution to the optimization problem - each with its own 'inertia' which is a step size and direction. At each iteration the particles move in a direction determined by linear combination of its inertia and the global best solution found so far. After multiple iterations of this, the algorithm will terminate when the particles have converged to an optimum.\n",
    "\n",
    "The comparison of the PSO and the EM will be the focus of the remainder of the paper. In the following section, we present the respective implementations of the methods. Then, we bring forth two ways of performing the compaarison, namely by applying our implementations to the well known Old Faithful dataset. To further quantify our comparison, we also perform Monte Carlo simulation. We end our report with the discussion of our results and potential for future improvements and research \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing our algorithms in Python. What follows is the mathematical description for each procedure. We make the important note that due to time constraints, we assume a priori knowledge of the true value for $\\sigma^{2}$. For EM, we follow the formulas provided by Smyth [2] in his notes for finite mixture models:\n",
    "\n",
    "We begin with an intial guess for the $\\pi$, $\\mu$, and $\\sigma^{2}$ parameters.\n",
    "\n",
    "The Expectation (E) step:\n",
    "\n",
    "$$w_{i,1} = \\frac{\\pi\\phi(x|\\mu_{1},\\sigma_{1}^{2})}{\\pi\\phi(x|\\mu_{1},\\sigma_{1}^{2}) +(1-\\pi)\\phi(x|\\mu_{2},\\sigma_{2}^{2})}$$\n",
    "\n",
    "and the component membership weights for the second component are $w_{i,2} = 1 - w_{i,1}$.\n",
    "\n",
    "Given the weights from the E step, we can update our estimates first by estimating the number of data points in the first component by $\\hat{n_{1}} = \\sum_{i=0}^{n}x_{i}w_{1,i}$, the weight of the first component by $\\hat{\\pi} = \\frac{\\hat{n_{1}}}{n}$, the mean of the first component $\\hat{\\mu_{1}} = \\frac{1}{\\hat{n}_{1}}\\sum_{i=1}^{n}x_{i}w_{1,i}$, and the mean of the second component $\\hat{\\mu_{2}} = \\frac{1}{n-\\hat{n}_{1}}\\sum_{i=1}^{n}x_{i}(1-w_{1,i})$. Note that the weight for the second component is $(1-\\pi)$ and that we do not include the calculation for the variance as we are assumping this parameter is known due to time constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Particle Swarm Optimization algorithm is described as follows [1]:\n",
    "\n",
    "1. Initialize $N$ particles, each with random velocity $\\vec{v}_{i}$ and parameter values $\\vec{p}_{i} =(\\hat{\\mu_{1}},\\hat{\\mu_{2}},\\hat{\\pi)}$.\n",
    "2. Define each particle's fitness to be the log-likelihood function $l$ evaluated at those parameter values.\n",
    "3. Define $\\vec{g}_{best}$ to be the highest scoring position amongst all particles, and $\\vec{p}_{best}$ to be each particle's personal best scoring position.\n",
    "4. Update all particles using the following formula: $$ \\vec{v}_{i+1} = \\vec{v}_{i} + c_{1}r_{1}(\\vec{p}_{best} - \\vec{p}_{i}) + c_{2}r_{2}(\\vec{p}_{best} - \\vec{p}_{i})$$    \n",
    "where $c_{1},c_{2}$ are constants and $r_{1},r_{2} \\sim U(0,1)$\n",
    "5. Repeat steps 2 to 4 until a termination condition is met.\n",
    "\n",
    "With the implemented algorithms, we wish to evaluate them in two ways. First, we will run each algorithm on the dataset consisting of the eruption times of the Old Faithful geyser. Then we will use the estimated parameters to create a method to generate from this distribution and compare the plots of the dataset density to a plot of the density of a sample simulated from our method. To evaluate PSO we will compare the plot it produces with that of EM and the true dataset. The second method of evaluation is via Monte Carlo simulation. We will randomly generate multiple datasets from mixture distributions and run our algorithms on these simulated datasets. After running the simulation multiple times we will calculate the expected error between the estimate and the true parameter value, given by $E[|\\hat{\\theta} - \\theta|]$. Please see the appendix for the R and Python code used for the experiments. In the next sections, we examine the results of our work and discuss the outcomes. \n",
    "\n",
    "To simplify our testing we imposed some restrictions to the parameters $(\\mu_{1},\\mu_{2},\\pi)$ used to simulate the datasets. The ranges $\\mu_{1} \\in [0,10],\\mu_{2} \\in [11,20],\\pi \\in [0.61,0.89]$ were guaranteed to yield distributions with two distinct means and sufficient samples from each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - Single Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - Multiple Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our analysis show that while PSO does sometimes make reasonable estimations for the parameters of the Gaussian mixture model, it currently does not beat EM. This is made clear by the higher mean absolute differences and by the graphical comparison. It is reasonable to conclude that the well known deterministic approach of EM is superior to the stochastic PSO. Of course, there is room for improvement in the implementation of the PSO algorithm. Future research could focus on spending more time tuning the conditions of the algorithm, such as the number of particles used, the number of iterations, as well as changing the weightings of the velocities. If exploring new possibilities for these values yields better results, then a next step would be to allow the algorithm to also estimate the variances of the components. Further enhancements would be to extend the algorithm to estimate a model with more than two components and to estimate multivariate rather than univarite models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Kennedy, J. and Eberhart, R. (1995). Particle swarm optimization. Proceedings of IEEE International Conference on Neural Networks. IV. pp. 1942–1948.   \n",
    "[2] Smyth, P. (2016). The EM algorithm for Gaussian Mixtures. Course notes - CS274A. University of California, Irvine.   \n",
    "[3] Christian, R.P. and Casella, G. (2010). The EM Algorithm. In Introducing Monte Carlo Methods with R. New York, NY.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix A - EM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def phi(x,mean,var):\n",
    "\tk = math.sqrt(2*math.pi*var)\n",
    "\tp = math.exp((x-mean)*(x-mean)/(-2*var))\n",
    "\treturn p/k\n",
    "def mix_dist(x,pi,mean1,mean2,var1,var2):\n",
    "\tcomp1 = pi*phi(x,mean1,var1)\n",
    "\tcomp2 = (1-pi)*phi(x,mean2,var2)\n",
    "\treturn comp1+comp2\n",
    "def gaussian_likelihood(x,pi,mean1,mean2,var1,var2):\n",
    "\t# x is a vector of data\n",
    "\tn = len(x)\n",
    "\tp = 0\n",
    "\tfor i in range(0,n):\n",
    "\t\tp = p + math.log(mix_dist(x[i],pi,mean1,mean2,var1,var2)) \n",
    "\treturn p\n",
    "\n",
    "\n",
    "def em(data,truSd,max_iter=20000):\n",
    "\t#mu1 = 0\n",
    "\t#mu2 = 0\n",
    "\tsd1 = 1#truSd#1\n",
    "\tsd2 = 1#truSd#1\n",
    "\tsprd = max(data) - min(data)\n",
    "\tmu1 = min(data)+0.25*sprd\n",
    "\tmu2 = min(data)+0.75*sprd\n",
    "\tpi1 = 0.5\n",
    "\tpi2 = 1 - pi1\n",
    "\tn = len(data)\n",
    "\t\n",
    "\tdef prob(x,mu,sd):\n",
    "\t\tnator = np.vectorize(math.exp)(((x-mu)*(x-mu))/(-2*sd*sd))\n",
    "\t\tdator = math.sqrt(2*math.pi)*sd\n",
    "\t\treturn (nator/dator)\n",
    "\t\n",
    "\tlogL = gaussian_likelihood(data,pi1,mu1,mu2,sd1*sd1,sd2*sd2)\n",
    "\t\n",
    "\tfor i in range(0,max_iter):\n",
    "\t\tp1 = prob(data,mu1,sd1)\n",
    "\t\tp2 = prob(data,mu2,sd2)\n",
    "\t\tw = (pi1*p1) / ((pi1*p1)+(pi2*p2))\n",
    "\t\tn1 = np.sum(w)\n",
    "\t\tn2 = n - n1\n",
    "\t\tpi1 = n1 / n\n",
    "\t\tpi2 = 1 - pi1\n",
    "\n",
    "\t\tmu1 = (1/n1)*np.sum(w*data)\n",
    "\t\tmu2 = (1/n2)*np.sum((1-w)*data)\n",
    "\t\t\n",
    "\t\tnewLogL = gaussian_likelihood(data,pi1,mu1,mu2,sd1*sd1,sd2*sd2)\n",
    "\t\t\n",
    "\t\tif abs(newLogL-logL) < 0.0001:\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tlogL = newLogL\n",
    "\n",
    "\t\tsd1 = math.sqrt(np.sum(w*(data-mu1)*(data-mu1)) / n1   )\n",
    "\t\tsd2 = math.sqrt((1/n2)*np.sum((1-w)*(data-mu2)*(data-mu2)))\n",
    "\t\t\n",
    "\tprint(str(mu1))    \n",
    "\tprint(str(pi1))\n",
    "\tprint(str(sd1))\n",
    "\tprint(str(mu2)) # \n",
    "\tprint(str(pi2)) # did not change\n",
    "\tprint(str(sd2)) # correct\n",
    "\n",
    "\n",
    "\n",
    "# Get parameters from call:\n",
    "sed = int(sys.argv[1])\n",
    "mu1 = float(sys.argv[2])\n",
    "mu2 = float(sys.argv[3])\n",
    "sd1 = float(sys.argv[4])\n",
    "sd2 = float(sys.argv[5])\n",
    "pi = float(sys.argv[6])\n",
    "n1 = int(200*pi) \n",
    "n2 = 200-n1\n",
    "\t\n",
    "np.random.seed(seed=sed)\n",
    "x1 = np.random.normal(mu1,sd1,n1)\n",
    "x2 = np.random.normal(mu2,sd2,n2)\n",
    "x = np.append(x1,x2)\n",
    "\n",
    "em(x,sd1)\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B - PSO Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from operator import attrgetter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Particle:\n",
    "\tdef __init__(self,position,velocity):\n",
    "\t\tself.position = position\n",
    "\t\tself.velocity = velocity\n",
    "\t\tself.pbest = position\n",
    "\t\tself.current_fitness = 0\n",
    "\t\tself.best_fitness = 0\n",
    "\t\tself.num_parameters = len(self.position)\n",
    "\tdef update_position(self,gbest,\n",
    "\t\t\t\t\t\tuse_boundary=False,\n",
    "\t\t\t\t\t\tlower_bound=None,\n",
    "\t\t\t\t\t\tupper_bound=None):              \n",
    "\t\tc1 = 1\n",
    "\t\tc2 = 1\n",
    "\t\tr1 = np.random.rand(1)\n",
    "\t\tr2 = np.random.rand(1)\n",
    "\t\tdelta = c1*r1*(self.pbest - self.position) + c2*r2*(gbest - self.position)\n",
    "\t\t\n",
    "\t\tupper_bound=np.zeros(self.num_parameters),\n",
    "\t\tlower_bound=np.zeros(self.num_parameters)\n",
    "\t\t\n",
    "\t\tcond1 = self.velocity + delta <= upper_bound\n",
    "\t\tcond2 = self.velocity + delta >= lower_bound\n",
    "\t\t\n",
    "\t\tif (use_boundary and cond1.all() and cond2.all()) or use_boundary==False:\n",
    "\t\t\tself.velocity = self.velocity + delta\n",
    "\t\t\tself.position = self.position + self.velocity\n",
    "\tdef _phi(self,x,mean,var):\n",
    "\t\tk = math.sqrt(2*math.pi*var)\n",
    "\t\tp = math.exp((x-mean)*(x-mean)/(-2*var))\n",
    "\t\treturn p/k\n",
    "\tdef _mix_dist(self,x,pi,mean1,mean2,var1,var2):\n",
    "\t\tcomp1 = pi*self._phi(x,mean1,var1)\n",
    "\t\tcomp2 = (1-pi)*self._phi(x,mean2,var2)\n",
    "\t\treturn comp1+comp2\n",
    "\tdef _gaussian_likelihood(self,x,pi,mean1,mean2,var1,var2):\n",
    "\t\t# x is a vector of data\n",
    "\t\tn = len(x)\n",
    "\t\tp = 0\n",
    "\t\tfor i in range(0,n):\n",
    "\t\t\tp = p + math.log(self._mix_dist(x[i],pi,mean1,mean2,var1,var2)) \n",
    "\t\treturn p\n",
    "\tdef calculate_fitness(self,x,pi,mean1,mean2,var1,var2):\n",
    "\t\tself.current_fitness = self._gaussian_likelihood(x,pi,mean1,mean2,var1,var2)\n",
    "\t\tif self.current_fitness > self.best_fitness or self.best_fitness == 0:\n",
    "\t\t\tself.pbest = self.position\n",
    "\t\t\tself.best_fitness = self.current_fitness\n",
    "\n",
    "\t\t\t\n",
    "def pso(x,truSig,truPi,num_particles=50,num_iter=100):\n",
    "\tN = num_particles\n",
    "\titerations = num_iter\n",
    "\n",
    "\tdata_min = min(x)\n",
    "\tdata_max = max(x)\n",
    "\tlower_boundary = np.array([0,data_min,data_min,0,0])\n",
    "\tupper_boundary = np.array([1,data_max,data_max,10,10])    \n",
    "\tparticles = [0] * N\n",
    "\ttolerance = 0.001\n",
    "\n",
    "\t# random intialization of particles\n",
    "\n",
    "\tnp.random.seed(69)\n",
    "\n",
    "\t# estimating a weight parameter and mu parameter for each of the two gaussian components\n",
    "\t# posn[0] = weight\n",
    "\t# posn[1] = mu1\n",
    "\t# posn[2] = mu2\n",
    "\n",
    "\tfor p in range(N):    \n",
    "\t\trand_pi = np.random.uniform(0,1)\n",
    "\t\trand_mean1 = np.random.uniform(data_min,data_max)\n",
    "\t\trand_mean2 = np.random.uniform(data_min,data_max)\n",
    "\t\tsigma1 = truSig\n",
    "\t\tsigma2 = truSig\n",
    "\t\trand_posn = np.array([rand_pi,rand_mean1,rand_mean2])\n",
    "\t\trand_velocity = np.array([np.random.uniform(0,1),\n",
    "\t\t\t\t\t\t\t\t np.random.uniform(0,1),\n",
    "\t\t\t\t\t\t\t\t np.random.uniform(0,1)])\n",
    "\t\t\n",
    "\t\tparticles[p] = Particle(rand_posn,rand_velocity)\n",
    "\t\tparticles[p].calculate_fitness(x,rand_pi,rand_mean1,rand_mean2,sigma1,sigma2)\n",
    "\n",
    "\tgbest = max(particles,key=attrgetter('best_fitness'))    \n",
    "\n",
    "\t# repeat until convergence\n",
    "\tfor i in range(iterations):\n",
    "\t\tfor p in particles:\n",
    "\t\t\tp.update_position(gbest.position,True,lower_boundary,upper_boundary)\n",
    "\n",
    "\t\t\tparams = p.position\n",
    "\t\t\tpi = params[0]\n",
    "\t\t\tmean1 = params[1]\n",
    "\t\t\tmean2 = params[2]\n",
    "\t\t\tsigma1 = truSig\n",
    "\t\t\tsigma2 = truSig\n",
    "\t\t\tp.calculate_fitness(x,pi,mean1,mean2,sigma1*sigma1,sigma2*sigma2)\n",
    "\t\t\tgbest = max(particles,key=attrgetter('best_fitness'))\n",
    "\t\n",
    "\t# Output results:\n",
    "\tif abs(truPi-gbest.pbest[0]) < abs(truPi-(1-gbest.pbest[0])):  \n",
    "\t\tprint(gbest.pbest[1]) # Est mu1\n",
    "\t\tprint(gbest.pbest[0]) # Est pi1\n",
    "\t\tprint(\"1\")\t\n",
    "\t\tprint(gbest.pbest[2]) # Est mu2\n",
    "\t\tprint(1-gbest.pbest[0]) # Est pi2\n",
    "\t\tprint(\"1\")\n",
    "\telse:\n",
    "\t\tprint(gbest.pbest[2]) # Est mu1\n",
    "\t\tprint(1-gbest.pbest[0]) # Est pi1\n",
    "\t\tprint(\"1\")\t\n",
    "\t\tprint(gbest.pbest[1]) # Est mu2\n",
    "\t\tprint(gbest.pbest[0]) # Est pi1\n",
    "\t\tprint(\"1\")\n",
    "\t\n",
    "\t#print(gbest.best_fitness)\n",
    "\treturn 0\n",
    "\n",
    "\t\n",
    "sed = int(sys.argv[1])\n",
    "mu1 = float(sys.argv[2])\n",
    "mu2 = float(sys.argv[3])\n",
    "sd1 = float(sys.argv[4])\n",
    "sd2 = float(sys.argv[5])\n",
    "pi = float(sys.argv[6])\n",
    "n1 = int(200*pi) \n",
    "n2 = 200-n1\n",
    "\t\n",
    "np.random.seed(seed=sed)\n",
    "x1 = np.random.normal(mu1,sd1,n1)\n",
    "x2 = np.random.normal(mu2,sd2,n2)\n",
    "x = np.append(x1,x2)\n",
    "\n",
    "pso(x,sd1,pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C - Old Faithful Analysis Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "# Function to simulate dataset from a \n",
    "# two component gaussian mxiture model\n",
    "rmixture <- function(n,mu1, mu2, sd1, sd2, pi) \n",
    "    u <- runif(n)\n",
    "    r <- numeric(n)\n",
    "    \n",
    "    for(i in seq(1,n)) {\n",
    "        if(u[i] <= pi) {\n",
    "            r[i] = rnorm(1, mean = mu1, sd = sd1)\n",
    "        } else {\n",
    "            r[i] = rnorm(1, mean = mu2, sd = sd2)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return(r)\n",
    "}\n",
    "\n",
    "# Initialize EM values\n",
    "seed <- 23\n",
    "mu1 <- 0\n",
    "mu2 <- 0\n",
    "sd1 <- 1\n",
    "sd2 <- 1\n",
    "pi <- 0.7\n",
    "params <- paste(seed,mu1,mu2,sd1,sd2,pi)\n",
    "\n",
    "x <- faithful[,2]\n",
    "write.csv(x, \"waittime.csv\", row.names = FALSE)\n",
    "d = density(x)\n",
    "plot(d,lwd=4)\n",
    "\n",
    "em_call <- paste(\"python faithfulEM.py\",params)\n",
    "emResults <- shell(em_call,intern=TRUE)\n",
    "results <- emResults[1:(length(emResults))]\n",
    "estMu1EM <- as.numeric(results[1])\n",
    "estPi1EM <- as.numeric(results[2])\n",
    "estSd1EM <- as.numeric(results[3])\n",
    "estMu2EM <- as.numeric(results[4])\n",
    "estPi2EM <- as.numeric(results[5])\n",
    "estSd2EM <- as.numeric(results[6])\n",
    "\n",
    "d = density(rmixture(272,estMu1EM,estMu2EM,estSd1EM,estSd2EM,estPi1EM )  )\n",
    "lines(d,col=2,lwd=4,lty=3)\n",
    "\n",
    "# Initialize PSO using sigmas from EM\n",
    "seed <- 23\n",
    "mu1 <- 0\n",
    "mu2 <- 0\n",
    "sd1 <- estSd1EM\n",
    "sd2 <- estSd2EM\n",
    "pi <- estPi1EM\n",
    "params <- paste(seed,mu1,mu2,sd1,sd2,pi)\n",
    "\n",
    "# True density of old faithful\n",
    "x <- faithful[,2]\n",
    "write.csv(x, \"waittime.csv\", row.names = FALSE)\n",
    "d = density(x)\n",
    "plot(d,lwd=4)\n",
    "\n",
    "# Plot density generated from PSO\n",
    "em_call <- paste(\"python faithfulPSO.py\",params)\n",
    "emResults <- shell(em_call,intern=TRUE)\n",
    "results <- emResults[1:(length(emResults))]\n",
    "estMu1PSO <- as.numeric(results[1])\n",
    "estPi1PSO <- as.numeric(results[2])\n",
    "estSd1PSO <- as.numeric(results[3])\n",
    "estMu2PSO <- as.numeric(results[4])\n",
    "estPi2PSO <- as.numeric(results[5])\n",
    "estSd2PSO <- as.numeric(results[6])\n",
    "\n",
    "d = density(rmixture(272,estMu1PSO,estMu2PSO,estSd1EM,estSd2EM,estPi1PSO )  )\n",
    "lines(d,col=2,lty=3,lwd=4)   \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix D - Monte Carlo Analysis Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "# Monte Carlo simulation\n",
    "M <- 1000\n",
    "\n",
    "truMu1 <- numeric(M)\n",
    "truPi1 <- numeric(M)\n",
    "truSd1 <- numeric(M)\n",
    "truMu2 <- numeric(M)\n",
    "truPi2 <- numeric(M)\n",
    "truSd2 <- numeric(M)\n",
    "\n",
    "estMu1EM <- numeric(M)\n",
    "estPi1EM <- numeric(M)\n",
    "estSd1EM <- numeric(M)\n",
    "estMu2EM <- numeric(M)\n",
    "estPi2EM <- numeric(M)\n",
    "estSd2EM <- numeric(M)\n",
    "\n",
    "estMu1PSO <- numeric(M)\n",
    "estPi1PSO <- numeric(M)\n",
    "estSd1PSO <- numeric(M)\n",
    "estMu2PSO <- numeric(M)\n",
    "estPi2PSO <- numeric(M)\n",
    "estSd2PSO <- numeric(M)\n",
    "\n",
    "for(i in seq(1,M) ) {\n",
    "    seed <- round(runif(n=1,min=1,max=100))\n",
    "    mu1 <- round(runif(n=1,min=0,max=10),digits=2)\n",
    "    mu2 <- round(runif(n=1,min=11,max=20),digits=2)\n",
    "    sd1 <- round(runif(n=1,min=1,max=3),digits=2)\n",
    "    sd2 <- sd1\n",
    "    pi <- round(runif(n=1,min=0.61,max=0.89),digits=2)\n",
    "    params <- paste(seed,mu1,mu2,sd1,sd2,pi)\n",
    "   \n",
    "    truMu1[i] <- mu1\n",
    "    truMu2[i] <- mu2\n",
    "    truPi1[i] <- pi \n",
    "    truPi2[i] <- 1 - pi\n",
    "    truSd1[i] <- sd1\n",
    "    truSd2[i] <- sd2\n",
    "    \n",
    "    # call python EM script\n",
    "    em_call <- paste(\"python randomEM.py\",params)\n",
    "    emResults <- shell(em_call,intern=TRUE)\n",
    "    results <- emResults[1:(length(emResults))]\n",
    "    estMu1EM[i] <- as.numeric(results[1])\n",
    "    estPi1EM[i] <- as.numeric(results[2])\n",
    "    estSd1EM[i] <- as.numeric(results[3])\n",
    "    estMu2EM[i] <- as.numeric(results[4])\n",
    "    estPi2EM[i] <- as.numeric(results[5])\n",
    "    estSd2EM[i] <- as.numeric(results[6])\n",
    "    \n",
    "    # call python PSO script\n",
    "    pso_call <- paste(\"python randomPSO.py\",params)\n",
    "    psoResults <- shell(pso_call,intern=TRUE)\n",
    "    results <- psoResults[1:(length(psoResults))]\n",
    "    estMu1PSO[i] <- as.numeric(results[1])\n",
    "    estPi1PSO[i] <- as.numeric(results[2])\n",
    "    estSd1PSO[i] <- as.numeric(results[3])\n",
    "    estMu2PSO[i] <- as.numeric(results[4])\n",
    "    estPi2PSO[i] <- as.numeric(results[5])\n",
    "    estSd2PSO[i] <- as.numeric(results[6])\n",
    "}\n",
    "\n",
    "print(\"EM\")\n",
    "mean(abs(truMu1 - estMu1EM))\n",
    "mean(abs(truPi1 - estPi1EM))\n",
    "mean(abs(truMu2 - estMu2EM))\n",
    "mean(abs(truPi2 - estPi2EM))\n",
    "\n",
    "print(\"PSO\")\n",
    "mean(abs(truMu1 - estMu1PSO ))\n",
    "mean(abs(truPi1 - estPi1PSO ))\n",
    "mean(abs(truMu2 - estMu2PSO ))\n",
    "mean(abs(truPi2 - estPi2PSO ))\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
