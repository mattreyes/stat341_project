{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many statistical populations contain distinct subpopulations, where each one may posses its own density. This can result in a multimodal distribution which can not be modelled by a single density. Modelling mixtures of distributions has key applications in document topic analysis and financial crisis modelling.\n",
    "\n",
    "Gaussian Mixture Models, used to model subpopulations, are convex combinations of Gaussian densities. Parameters for this model can be estimated using the Expectation Maximization (EM) algorithm - a probablistic approach.\n",
    "\n",
    "Recent discoveries in optimization research have yielded genetic algorithms inspired by processes found in nature. One particular example is Particle Swarm Optimization, which we implement to perform maximum likelihood estimation. We compare this algorithm to the classical EM by applying them to the Old Faithful dataset and to simulated mixture densities.\n",
    "\n",
    "Particularly, we aim to compare this non-statistical approach (PSO) to the probablisitic approach (EM) to estimate densities for the univariate case where there are two distinct groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, datasets are generated from process which can be represented as a combination of Gaussian distributions. The parameters of the distribution that generates these samples can offer insights into properties of the population, such as the subpopulation means and the weights of each mixture component. Concretely, this mixture distribution represents a decomposition of a single density into a convex combination of many. \n",
    "\n",
    "We formally define a Gaussian Mixture Model pdf $f$ with two components as\n",
    "\n",
    "$$\n",
    "f(x\\space|\\space\\mu_{1},\\sigma_{1}^{2},\\mu_{2},\\sigma_{2}^{2}) = \\pi\\phi(x\\space|\\space\\mu_{1},\\sigma_{1}^{2}) + (1-\\pi)\\phi(x\\space|\\space\\mu_{2},\\sigma_{2}^{2})\n",
    "$$\n",
    "\n",
    "where $\\phi_{1},\\phi_{2}$ are the Gaussian pdf and the weight $\\pi \\in [0,1]$.\n",
    "\n",
    "This formulation describes two Gaussian components, each with a weight and their respective parameters $\\mu$ and $\\sigma_{2}$ being their mean and variance respectively.\n",
    "\n",
    "We often do not know these parameters of the population given a sample dataset. At first glance, an approach would be to determine the log-likelihood function, yielding\n",
    "\n",
    "\n",
    "$$\n",
    "l(\\mu_{1},\\sigma_{1}^{2},\\mu_{2},\\sigma_{2}^{2}|x) = \n",
    "\\sum_{i=1}^{n}[\\ln(\\pi\\phi(x_{i}|\\mu_{1},\\sigma_{1}^{2}) + (1-\\pi)\\phi(x_{i}|\\mu_{2},\\sigma_{2}^{2}))]\n",
    "$$\n",
    "\n",
    "Maximum likelihood estimation by setting the score function to zero would require solving a multivariate optimization problem. This problem is challenging due to multiple variables, the complexity of the density $\\phi$, and the summation of these. Thus numerical algorithms are required to find an approximate solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most widely used approach is the Expectation Maximization (EM) algorithm (CITE TEXTBOOK HERE). \n",
    "It is a deterministic optimization technique derived from minorize-maximization (MM) methods. We say the log-likelihood function is maximized by by iteratively maximizing simpler functions. \n",
    "\n",
    "(?? ADD ALGO LATER???)\n",
    "\n",
    "\n",
    "\n",
    "PSO = inspiration etc\n",
    "\n",
    "We propose to compare this to the Particle Swarm Optimization - a stochastic optimization algorithm first described by Kennedy (CITE HIM) in 1995. It draws inspirations from behaviour seen in crowds of lifeforms such as school of fish or flocks of birds. The main idea is to begin with an initial 'swarm' of particles with values randomly chosen from the search space. Each particle represents a candidate solution to the optimization problem - each with its own 'inertia' which is a step size and direction. At each iteration the particles move in a direction determined by linear combination of its inertia and the global best solution found so far. After multiple iterations of this, the algorithm will terminate when the particles have converged to an optimum.\n",
    "\n",
    "The comparison of the PSO and the EM will be the focus of the remainder of the paper. In the following section, we present the respective implementations of the methods. Then, we bring forth two ways of performing the compaarison, namely by applying our implementations to the well known Old Faithful dataset. To further quantify our comparison, we also perform Monte Carlo simulation. We end our report with the discussion of our results and potential for future improvements and research \n",
    "\n",
    "It was first described by Dempster et al. in 1977. \n",
    "\n",
    "Hype about non-statistical algos \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "example figure here? :O\n",
    "\n",
    "- comparisons between these two methods via repeated applications of thhe algooirthm to simulated datasets\n",
    "- mention PSO typically used for opt problems and we wish to see how it performs in statistical density estimation \n",
    "- non-statistical methods for machine learning and estimation are gaining traction and we wish to test that shit out (cite XXX)\n",
    "\n",
    "- in the remainder of this paper we implement 1995's PSO and X articles EM (textbook) and \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We formally define a Gaussian Mixture Model as\n",
    "\n",
    "$$\n",
    "XXX\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- compare PSO and EM algos and discuss implemetnation\n",
    "\t- math behind PSO and how the vector is updated\n",
    "\t- EM formulas\n",
    "\n",
    "- show math for likelihood function\n",
    "- implemntation details\n",
    "- will we quantify the comparision using\n",
    "- assumptions/restrictions on our simulated params: only two mixtures\n",
    "- monte carlo thing (simulated data)\n",
    "\t- generate multiple datasets from known mixture distributions\n",
    "\t- we can measure accuracy/error by comparing the resulting params to the the true ones\n",
    "\n",
    "- how we compare\n",
    "- discuss error measurements\n",
    "\n",
    "- application of analysis to a real dataset (faithful data)\n",
    "\t- compare both \n",
    "\t- plot the estimated densities vs the original dataset\n",
    "\t- simulate data from estimated mixtures and compare to original dataset - compare densities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Single Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Multiple Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
