{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many statistical populations contain distinct subpopulations, where each one may posses its own density. This can result in a multimodal distribution which can not be modelled by a single density. Modelling mixtures of distributions has key applications in document topic analysis and financial crisis modelling.\n",
    "\n",
    "Gaussian Mixture Models, used to model subpopulations, are convex combinations of Gaussian densities. Parameters for this model can be estimated using the Expectation Maximization (EM) algorithm - a probablistic approach.\n",
    "\n",
    "Recent discoveries in optimization research have yielded genetic algorithms inspired by processes found in nature. One particular example is Particle Swarm Optimization, which we implement to perform maximum likelihood estimation. We compare this algorithm to the classical EM by applying them to the Old Faithful dataset and to simulated mixture densities.\n",
    "\n",
    "Particularly, we aim to compare this non-statistical approach (PSO) to the probablisitic approach (EM) to estimate densities for the univariate case where there are two distinct groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, datasets are generated from process which can be represented as a combination of Gaussian distributions. The parameters of the distribution that generates these samples can offer insights into properties of the population, such as the subpopulation means and the weights of each mixture component. Concretely, this mixture distribution represents a decomposition of a single density into a convex combination of many. \n",
    "\n",
    "We formally define a Gaussian Mixture Model pdf $f$ with two components as\n",
    "\n",
    "$$\n",
    "f(x\\space|\\space\\mu_{1},\\sigma_{1}^{2},\\mu_{2},\\sigma_{2}^{2}) = \\pi\\phi(x\\space|\\space\\mu_{1},\\sigma_{1}^{2}) + (1-\\pi)\\phi(x\\space|\\space\\mu_{2},\\sigma_{2}^{2})\n",
    "$$\n",
    "\n",
    "where $\\phi_{1},\\phi_{2}$ are the Gaussian pdf and the weight $\\pi \\in [0,1]$.\n",
    "\n",
    "This formulation describes two Gaussian components, each with a weight and their respective parameters $\\mu$ and $\\sigma_{2}$ being their mean and variance respectively.\n",
    "\n",
    "We often do not know these parameters of the population given a sample dataset. At first glance, an approach would be to determine the log-likelihood function, yielding\n",
    "\n",
    "\n",
    "$$\n",
    "l(\\mu_{1},\\sigma_{1}^{2},\\mu_{2},\\sigma_{2}^{2}|x) = \n",
    "\\sum_{i=1}^{n}[\\ln(\\pi\\phi(x_{i}|\\mu_{1},\\sigma_{1}^{2}) + (1-\\pi)\\phi(x_{i}|\\mu_{2},\\sigma_{2}^{2}))]\n",
    "$$\n",
    "\n",
    "Maximum likelihood estimation by setting the score function to zero would require solving a multivariate optimization problem. This problem is challenging due to multiple variables, the complexity of the density $\\phi$, and the summation of these. Thus numerical algorithms are required to find an approximate solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most widely used approach is the Expectation Maximization (EM) algorithm (CITE TEXTBOOK HERE). \n",
    "It is a deterministic optimization technique derived from minorize-maximization (MM) methods. We say the log-likelihood function is maximized by by iteratively maximizing simpler functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PSO = inspiration etc\n",
    "\n",
    "We propose to compare this to the Particle Swarm Optimization - a stochastic optimization algorithm first described by Kennedy (CITE HIM) in 1995. It draws inspirations from behaviour seen in crowds of lifeforms such as school of fish or flocks of birds. The main idea is to begin with an initial 'swarm' of particles with values randomly chosen from the search space. Each particle represents a candidate solution to the optimization problem - each with its own 'inertia' which is a step size and direction. At each iteration the particles move in a direction determined by linear combination of its inertia and the global best solution found so far. After multiple iterations of this, the algorithm will terminate when the particles have converged to an optimum.\n",
    "\n",
    "The comparison of the PSO and the EM will be the focus of the remainder of the paper. In the following section, we present the respective implementations of the methods. Then, we bring forth two ways of performing the compaarison, namely by applying our implementations to the well known Old Faithful dataset. To further quantify our comparison, we also perform Monte Carlo simulation. We end our report with the discussion of our results and potential for future improvements and research \n",
    "\n",
    "It was first described by Dempster et al. in 1977. \n",
    "\n",
    "Hype about non-statistical algos \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "example figure here? :O\n",
    "\n",
    "- comparisons between these two methods via repeated applications of thhe algooirthm to simulated datasets\n",
    "- mention PSO typically used for opt problems and we wish to see how it performs in statistical density estimation \n",
    "- non-statistical methods for machine learning and estimation are gaining traction and we wish to test that shit out (cite XXX)\n",
    "\n",
    "- in the remainder of this paper we implement 1995's PSO and X articles EM (textbook) and \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by implementing our algorithms in Python. What follows is the mathematical description for each procedure. We make the important note that due to time constraints, we assume a priori knowledge of the true value for $\\sigma^2$. For EM, we follow the formulas provided by Smyth in his notes for finite mixture models:\n",
    "\n",
    "We begin with an intial guess for the $\\pi$, $\\mu$'s, and ${\\sigma}^2$'s.\n",
    "\n",
    "The Expectation (E) step:\n",
    "\n",
    "For each data point $x_{i}$ $1 \\leq i \\leq n $, we compute its component membership weight that it is in the first component\n",
    "\n",
    "$$\n",
    "w_{i,1} = \\frac{ \\hat{\\pi}\\phi(x_{i}|\\hat{\\mu_{1}},\\hat{\\sigma_{1}}^2)}{  \\hat{\\pi}\\phi(x_{i}|\\hat{\\mu_{1}},\\hat{\\sigma_{1}}^2) + (1-\\hat{\\pi})\\phi(x_{i}|\\hat{\\mu_{2}},\\hat{\\sigma_{2}}^2)  }\n",
    "$$\n",
    "and the component membership weights for the second component are\n",
    "\n",
    "$$\n",
    "w_{i,2} = 1 - w_{i,1}\n",
    "$$\n",
    "\n",
    "The Maximization (M) step:\n",
    "\n",
    "Given the weights from the E step, we can update our estimates first by estimating the number of data points in the first component by $ \\hat{n_{1}} = \\sum_{i=0}^{n}w_{i,1}$, the weight of the first componenent by \n",
    "$\\hat{\\pi} = \\frac{\\hat{n_{1}}}{n}$, the mean of the first component  $\\hat{\\mu_{1}} = \\frac{1}{\\hat{n_{1}}}\\sum_{i=1}^{n}x_{i}w_{1,i}$ and the mean of the second component $\\hat{\\mu_{2}} = \\frac{1}{\\hat{n-n_{1}}}\\sum_{i=1}^{n}x_{i}(1-w_{1,i})$. We note that the weight for the second component is $(1-\\pi)$ and that we do not include the calculation for the variance as we are assuming this parameter is known due to time constraints.\n",
    "\n",
    "The PSO algorithm is described as follow:\n",
    "\n",
    "INCLUDE FORMULATION HERE\n",
    "\n",
    "\n",
    "With the implemented algorithms, we which to evaluate them in two ways. First, we will run each algorithm on the dataset consisting of the eruption times of the Old Faithful geyser. Then we will use the estimated parameters to create a method to generate from this distribution and compare the plots of the dataset density to a plot of the density of a sample simulated from our method. To evaluate PSO we will compare the plot it produces with that of EM and the true dataset. The second method of evaluation is via Monte Carlo simulation. We will randomly generate multiple datasets from mixture distributions and run our algorithms on these simulated datasets. After running the simulation multiple times we may calculate the expected error between the estimate and the true parameter value. Please see the appendix for the R and Python code used for the experiments. In the next sections, we examine the results of our work and discuss the outcomes. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- compare PSO and EM algos and discuss implemetnation\n",
    "\t- math behind PSO and how the vector is updated\n",
    "\t- EM formulas\n",
    "\n",
    "- show math for likelihood function\n",
    "- implemntation details\n",
    "- will we quantify the comparision using\n",
    "- assumptions/restrictions on our simulated params: only two mixtures\n",
    "- monte carlo thing (simulated data)\n",
    "\t- generate multiple datasets from known mixture distributions\n",
    "\t- we can measure accuracy/error by comparing the resulting params to the the true ones\n",
    "\n",
    "- how we compare\n",
    "- discuss error measurements\n",
    "\n",
    "- application of analysis to a real dataset (faithful data)\n",
    "\t- compare both \n",
    "\t- plot the estimated densities vs the original dataset\n",
    "\t- simulate data from estimated mixtures and compare to original dataset - compare densities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Single Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Multiple Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Kennedy, J. and Eberhart, R. (1995). Particle swarm optimization. Proceedings of IEEE International Conference on Neural Networks. IV. pp. 1942–1948. \n",
    "\n",
    "[2] Smyth, P. (2016). The EM algorithm for Gaussian Mixtures. Course notes - CS274A. University of California, Irvine.\n",
    "\n",
    "[3] Christian, R.P. and Casella, G. (2010). The EM Algorithm. In Introducing Monte Carlo Methods with R. New York, NY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
